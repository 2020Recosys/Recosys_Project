{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datetime\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tqdm import tqdm_notebook\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from keras.utils import to_categorical\n",
    "from eli5.sklearn import PermutationImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1308984798696364150\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 10057666703699374684\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.6/dist-packages (1.0.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.22.1)\r\n",
      "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.18.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.13.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_초순 = pd.read_csv('Y_초순.csv')\n",
    "Y_중순 = pd.read_csv('Y_중순.csv')\n",
    "Y_하순 = pd.read_csv('Y_하순.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>패션초순구입</th>\n",
       "      <th>가전제품초순구입</th>\n",
       "      <th>화장품초순구입</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20120712210FAAE99B43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201207175F12579F84D4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20120723EC3C68DA5E56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201207252C25DC618A00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012072582E08BEB79C6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>201407051A8A544E7F79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>20140705A1F86A14CC89</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>20140706952D03CA2CF1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>2014070991B7C8F3BBD9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>20140709E002C85D81AA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1063 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       UID  패션초순구입  가전제품초순구입  화장품초순구입\n",
       "0     20120712210FAAE99B43       1         1        0\n",
       "1     201207175F12579F84D4       0         0        1\n",
       "2     20120723EC3C68DA5E56       0         1        0\n",
       "3     201207252C25DC618A00       0         1        1\n",
       "4     2012072582E08BEB79C6       0         0        1\n",
       "...                    ...     ...       ...      ...\n",
       "1058  201407051A8A544E7F79       0         0        1\n",
       "1059  20140705A1F86A14CC89       0         1        0\n",
       "1060  20140706952D03CA2CF1       0         0        0\n",
       "1061  2014070991B7C8F3BBD9       0         0        0\n",
       "1062  20140709E002C85D81AA       0         1        0\n",
       "\n",
       "[1063 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_초순"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_초순 = pd.read_csv('초순_3d_array.csv')\n",
    "X_중순 = pd.read_csv('중순_3d_array.csv')\n",
    "X_하순 = pd.read_csv('하순_3d_array.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>94990</th>\n",
       "      <th>94991</th>\n",
       "      <th>94992</th>\n",
       "      <th>94993</th>\n",
       "      <th>94994</th>\n",
       "      <th>94995</th>\n",
       "      <th>94996</th>\n",
       "      <th>94997</th>\n",
       "      <th>94998</th>\n",
       "      <th>94999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1754</td>\n",
       "      <td>29010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>308</td>\n",
       "      <td>10185</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>314</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1302</td>\n",
       "      <td>29956</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1361</td>\n",
       "      <td>7985</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1  2  3  4  5  6  7  8  9  ...  94990  94991  94992  94993  \\\n",
       "0  1754  29010  0  0  0  0  1  0  0  0  ...      0      0      0      0   \n",
       "1   308  10185  0  0  0  0  1  0  0  0  ...      0      0      0      0   \n",
       "2    24    314  0  0  0  0  0  0  0  0  ...      0      0      0      0   \n",
       "3  1302  29956  0  0  0  0  0  0  0  1  ...      0      0      0      0   \n",
       "4  1361   7985  0  0  0  0  0  0  0  1  ...      0      0      0      0   \n",
       "\n",
       "   94994  94995  94996  94997  94998  94999  \n",
       "0      0      0      0      0      1      0  \n",
       "1      0      0      0      0      1      0  \n",
       "2      0      0      0      0      1      0  \n",
       "3      0      0      1      0      0      0  \n",
       "4      0      0      0      0      1      0  \n",
       "\n",
       "[5 rows x 95000 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_초순.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_초순 = np.asarray(X_초순)\n",
    "X_중순 = np.asarray(X_중순)\n",
    "X_하순 = np.asarray(X_하순)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_초순 = np.asarray(Y_초순.iloc[:, 1:])\n",
    "Y_중순 = np.asarray(Y_중순.iloc[:, 1:])\n",
    "Y_하순 = np.asarray(Y_하순.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_초순 = X_초순.reshape(1063, 5000, 19)\n",
    "X_중순 = X_중순.reshape(1063, 5000, 19)\n",
    "X_하순 = X_하순.reshape(1063, 5000, 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 가동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 초순"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X_초순, Y_초순, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = to_categorical(y_train)\n",
    "#y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#masking layer 추가(https://stackoverflow.com/questions/49670832/keras-lstm-with-masking-layer-for-variable-length-inputs)\n",
    "def get_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0., input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(LSTM(64,input_shape = input_shape))\n",
    "    #model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(32, activation= 'relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation= 'relu'))\n",
    "    model.add(Dense(4, activation='sigmoid'))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr= 0.001, rho = 0.9), metrics=['acc'])\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_with(input_shape, verbose, lr):\n",
    "\n",
    "    # Create the model using a specified hyperparameters.\n",
    "    model = get_model(input_shape)\n",
    "\n",
    "    # Train the model for a specified number of epochs.\n",
    "    optimizer = RMSprop(learning_rate=lr)\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the model with the train dataset.\n",
    "    model.fit(x=X_train,y= y_train ,epochs=1,\n",
    "              batch_size=64, verbose=verbose)\n",
    "\n",
    "    # Evaluate the model with the eval dataset.\n",
    "    score = model.evaluate(X_test,y= y_test ,steps=10, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    # Return the accuracy.\n",
    "\n",
    "    return score[1]\n",
    "from functools import partial\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "verbose = 1\n",
    "fit_with_partial = partial(fit_with, input_shape, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |    lr     |\n",
      "-------------------------------------\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 116s 72ms/step - loss: 1.2703 - accuracy: 0.3767\n",
      "Test loss: 0.10801259279251099\n",
      "Test accuracy: 4.716981053352356\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 4.717   \u001b[0m | \u001b[0m 0.004229\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 118s 73ms/step - loss: 1.0698 - accuracy: 0.4284\n",
      "Test loss: 0.10224945545196533\n",
      "Test accuracy: 4.296081364154816\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 4.296   \u001b[0m | \u001b[0m 0.007231\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 125s 78ms/step - loss: 1.4472 - accuracy: 0.2416\n",
      "Test loss: 0.14132689237594603\n",
      "Test accuracy: 3.294629752635956\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 3.295   \u001b[0m | \u001b[0m 0.000101\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 118s 73ms/step - loss: 1.1305 - accuracy: 0.4732\n",
      "Test loss: 0.09361357092857361\n",
      "Test accuracy: 5.703918933868408\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 5.704   \u001b[0m | \u001b[95m 0.003093\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 118s 73ms/step - loss: 1.1579 - accuracy: 0.3761\n",
      "Test loss: 0.106751549243927\n",
      "Test accuracy: 5.07982611656189\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 5.08    \u001b[0m | \u001b[0m 0.001553\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 127s 79ms/step - loss: 1.1653 - accuracy: 0.3817\n",
      "Test loss: 0.10989166498184204\n",
      "Test accuracy: 3.570391833782196\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 3.57    \u001b[0m | \u001b[0m 0.001014\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 120s 75ms/step - loss: 1.1504 - accuracy: 0.3350\n",
      "Test loss: 0.10425639152526855\n",
      "Test accuracy: 3.4397676587104797\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 3.44    \u001b[0m | \u001b[0m 0.001944\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 115s 72ms/step - loss: 1.0831 - accuracy: 0.4477\n",
      "Test loss: 0.09328081011772156\n",
      "Test accuracy: 5.3410738706588745\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 5.341   \u001b[0m | \u001b[0m 0.003521\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 119s 74ms/step - loss: 1.0481 - accuracy: 0.5093\n",
      "Test loss: 0.09408735036849976\n",
      "Test accuracy: 5.181422233581543\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 5.181   \u001b[0m | \u001b[0m 0.004028\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 116s 72ms/step - loss: 1.0610 - accuracy: 0.4259\n",
      "Test loss: 0.09999374747276306\n",
      "Test accuracy: 5.1233673095703125\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 5.123   \u001b[0m | \u001b[0m 0.005434\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 118s 74ms/step - loss: 1.0870 - accuracy: 0.4352\n",
      "Test loss: 0.09493661522865296\n",
      "Test accuracy: 5.37010133266449\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 5.37    \u001b[0m | \u001b[0m 0.002761\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 113s 70ms/step - loss: 1.0481 - accuracy: 0.4508\n",
      "Test loss: 0.09434975385665893\n",
      "Test accuracy: 5.13788104057312\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 5.138   \u001b[0m | \u001b[0m 0.006024\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 123s 76ms/step - loss: 0.9825 - accuracy: 0.4888\n",
      "Test loss: 0.08982491493225098\n",
      "Test accuracy: 5.471698045730591\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 5.472   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 117s 73ms/step - loss: 1.0131 - accuracy: 0.4695\n",
      "Test loss: 0.09198938608169556\n",
      "Test accuracy: 5.239477753639221\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 5.239   \u001b[0m | \u001b[0m 0.00948 \u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 120s 75ms/step - loss: 1.0015 - accuracy: 0.4795\n",
      "Test loss: 0.1206917405128479\n",
      "Test accuracy: 3.526850640773773\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 3.527   \u001b[0m | \u001b[0m 0.008739\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 120s 75ms/step - loss: 1.0884 - accuracy: 0.3418\n",
      "Test loss: 0.09792449474334716\n",
      "Test accuracy: 5.500725507736206\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 5.501   \u001b[0m | \u001b[0m 0.006538\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 116s 73ms/step - loss: 1.0650 - accuracy: 0.3910\n",
      "Test loss: 0.0906749963760376\n",
      "Test accuracy: 5.326560139656067\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 5.327   \u001b[0m | \u001b[0m 0.004927\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 118s 74ms/step - loss: 1.1015 - accuracy: 0.4284\n",
      "Test loss: 0.09757402539253235\n",
      "Test accuracy: 5.195935964584351\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 5.196   \u001b[0m | \u001b[0m 0.007925\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 115s 72ms/step - loss: 1.1517 - accuracy: 0.3711\n",
      "Test loss: 0.10731278657913208\n",
      "Test accuracy: 3.889695107936859\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 3.89    \u001b[0m | \u001b[0m 0.003107\u001b[0m |\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 114s 71ms/step - loss: 1.0671 - accuracy: 0.3979\n",
      "Test loss: 0.09564208984375\n",
      "Test accuracy: 5.3410738706588745\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 5.341   \u001b[0m | \u001b[0m 0.003081\u001b[0m |\n",
      "=====================================\n",
      "Iteration 0: \n",
      "\t{'target': 4.716981053352356, 'params': {'lr': 0.004228517846555483}}\n",
      "Iteration 1: \n",
      "\t{'target': 4.296081364154816, 'params': {'lr': 0.007231212485077366}}\n",
      "Iteration 2: \n",
      "\t{'target': 3.294629752635956, 'params': {'lr': 0.00010113231069171439}}\n",
      "Iteration 3: \n",
      "\t{'target': 5.703918933868408, 'params': {'lr': 0.003093092469055214}}\n",
      "Iteration 4: \n",
      "\t{'target': 5.07982611656189, 'params': {'lr': 0.0015528833190894193}}\n",
      "Iteration 5: \n",
      "\t{'target': 3.570391833782196, 'params': {'lr': 0.0010141520882110983}}\n",
      "Iteration 6: \n",
      "\t{'target': 3.4397676587104797, 'params': {'lr': 0.0019439760926389421}}\n",
      "Iteration 7: \n",
      "\t{'target': 5.3410738706588745, 'params': {'lr': 0.003521051197726173}}\n",
      "Iteration 8: \n",
      "\t{'target': 5.181422233581543, 'params': {'lr': 0.004027997994883633}}\n",
      "Iteration 9: \n",
      "\t{'target': 5.1233673095703125, 'params': {'lr': 0.005434285666633234}}\n",
      "Iteration 10: \n",
      "\t{'target': 5.37010133266449, 'params': {'lr': 0.002761237786148106}}\n",
      "Iteration 11: \n",
      "\t{'target': 5.13788104057312, 'params': {'lr': 0.006024380690034502}}\n",
      "Iteration 12: \n",
      "\t{'target': 5.471698045730591, 'params': {'lr': 0.01}}\n",
      "Iteration 13: \n",
      "\t{'target': 5.239477753639221, 'params': {'lr': 0.009480329785707536}}\n",
      "Iteration 14: \n",
      "\t{'target': 3.526850640773773, 'params': {'lr': 0.008738926243592307}}\n",
      "Iteration 15: \n",
      "\t{'target': 5.500725507736206, 'params': {'lr': 0.00653820192570272}}\n",
      "Iteration 16: \n",
      "\t{'target': 5.326560139656067, 'params': {'lr': 0.0049274385191790645}}\n",
      "Iteration 17: \n",
      "\t{'target': 5.195935964584351, 'params': {'lr': 0.007924726935973153}}\n",
      "Iteration 18: \n",
      "\t{'target': 3.889695107936859, 'params': {'lr': 0.0031069541240735595}}\n",
      "Iteration 19: \n",
      "\t{'target': 5.3410738706588745, 'params': {'lr': 0.0030806776442324412}}\n",
      "{'target': 5.703918933868408, 'params': {'lr': 0.003093092469055214}}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'lr': (1e-4, 1e-2)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=fit_with_partial,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=10, n_iter=10)\n",
    "\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main MOdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0211 11:33:46.560149 139754087118656 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_1 (Masking)          (None, 5000, 19)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                21504     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 24,163\n",
      "Trainable params: 24,163\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#masking layer 추가(https://stackoverflow.com/questions/49670832/keras-lstm-with-masking-layer-for-variable-length-inputs)\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0., input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(64,input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "#model.add(Glob)\n",
    "model.add(Dense(32, activation= 'relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation= 'relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr= 0.003, rho = 0.9))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0211 11:33:47.187179 139754087118656 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 744 samples, validate on 319 samples\n",
      "Epoch 1/30\n",
      " - 30s - loss: 0.8772 - val_loss: 0.8609\n",
      "Epoch 2/30\n",
      " - 30s - loss: 0.8719 - val_loss: 0.8590\n",
      "Epoch 3/30\n",
      " - 30s - loss: 0.8684 - val_loss: 0.8612\n",
      "Epoch 4/30\n",
      " - 30s - loss: 0.8641 - val_loss: 0.8613\n",
      "Epoch 5/30\n",
      " - 31s - loss: 0.8603 - val_loss: 0.8642\n",
      "Epoch 6/30\n",
      " - 44s - loss: 0.8567 - val_loss: 0.8673\n",
      "Epoch 7/30\n",
      " - 44s - loss: 0.8508 - val_loss: 0.8781\n",
      "Epoch 8/30\n",
      " - 44s - loss: 0.8471 - val_loss: 0.8847\n",
      "Epoch 9/30\n",
      " - 44s - loss: 0.8418 - val_loss: 0.8836\n",
      "Epoch 10/30\n",
      " - 44s - loss: 0.8360 - val_loss: 0.8873\n",
      "Epoch 11/30\n",
      " - 44s - loss: 0.8305 - val_loss: 0.8877\n",
      "Epoch 12/30\n",
      " - 44s - loss: 0.8230 - val_loss: 0.9069\n",
      "Epoch 13/30\n",
      " - 44s - loss: 0.8213 - val_loss: 0.8945\n",
      "Epoch 14/30\n",
      " - 44s - loss: 0.8181 - val_loss: 0.8982\n",
      "Epoch 15/30\n",
      " - 44s - loss: 0.8125 - val_loss: 0.9122\n",
      "Epoch 16/30\n",
      " - 44s - loss: 0.8027 - val_loss: 0.9166\n",
      "Epoch 17/30\n",
      " - 44s - loss: 0.8030 - val_loss: 0.9128\n",
      "Epoch 18/30\n",
      " - 44s - loss: 0.7869 - val_loss: 0.9296\n",
      "Epoch 19/30\n",
      " - 44s - loss: 0.7778 - val_loss: 0.9229\n",
      "Epoch 20/30\n",
      " - 44s - loss: 0.7794 - val_loss: 0.9227\n",
      "Epoch 21/30\n",
      " - 44s - loss: 0.7567 - val_loss: 0.9599\n",
      "Epoch 22/30\n",
      " - 44s - loss: 0.7688 - val_loss: 0.9463\n",
      "Epoch 23/30\n",
      " - 44s - loss: 0.7546 - val_loss: 0.9834\n",
      "Epoch 24/30\n",
      " - 44s - loss: 0.7560 - val_loss: 0.9897\n",
      "Epoch 25/30\n",
      " - 44s - loss: 0.7396 - val_loss: 1.0059\n",
      "Epoch 26/30\n",
      " - 44s - loss: 0.7258 - val_loss: 1.0107\n",
      "Epoch 27/30\n",
      " - 44s - loss: 0.7267 - val_loss: 1.0096\n",
      "Epoch 28/30\n",
      " - 44s - loss: 0.7169 - val_loss: 1.0406\n",
      "Epoch 29/30\n",
      " - 44s - loss: 0.7255 - val_loss: 1.0512\n",
      "Epoch 30/30\n",
      " - 44s - loss: 0.7020 - val_loss: 1.0931\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, batch_size=100, validation_data=(X_test, y_test), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "preds[preds>=0.5] = 1\n",
    "preds[preds<0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.39892101e-04, 1.27440691e-02, 3.19618285e-02],\n",
       "       [1.36760414e-01, 3.03606629e-01, 4.15020376e-01],\n",
       "       [2.07568288e-01, 3.12772036e-01, 1.51957542e-01],\n",
       "       [1.66997790e-01, 1.41199738e-01, 2.41556555e-01],\n",
       "       [1.37333989e-01, 3.75136554e-01, 4.44858789e-01],\n",
       "       [1.67945325e-02, 9.41620171e-02, 1.08144045e-01],\n",
       "       [2.00342119e-01, 7.37184286e-02, 8.55875909e-02],\n",
       "       [3.53851944e-01, 1.83645248e-01, 2.65966237e-01],\n",
       "       [1.92403525e-01, 3.06883156e-01, 6.82542026e-02],\n",
       "       [3.47204179e-01, 2.38875628e-01, 1.33252382e-01],\n",
       "       [1.24166816e-01, 3.58075500e-01, 3.28802913e-01],\n",
       "       [2.79790580e-01, 3.60766947e-01, 2.89808273e-01],\n",
       "       [2.45070010e-01, 3.50970984e-01, 2.53310233e-01],\n",
       "       [1.54566437e-01, 2.74843693e-01, 2.67844379e-01],\n",
       "       [8.91028047e-02, 1.08427733e-01, 1.75493926e-01],\n",
       "       [3.61856818e-03, 5.82008958e-02, 2.37311512e-01],\n",
       "       [1.64956778e-01, 1.20637596e-01, 1.96472794e-01],\n",
       "       [4.61695194e-02, 2.98529893e-01, 1.83056176e-01],\n",
       "       [2.20982909e-01, 2.74927139e-01, 2.81086624e-01],\n",
       "       [1.54139251e-01, 3.76053691e-01, 2.78331101e-01],\n",
       "       [2.46275872e-01, 3.05180490e-01, 2.80456543e-01],\n",
       "       [3.84512782e-01, 2.58149862e-01, 1.53679192e-01],\n",
       "       [1.67224705e-02, 1.03912294e-01, 2.26087242e-01],\n",
       "       [2.08822072e-01, 4.41787839e-01, 1.03774428e-01],\n",
       "       [6.26249313e-02, 4.04422224e-01, 4.86132801e-02],\n",
       "       [2.28225291e-01, 3.71968448e-01, 4.10468400e-01],\n",
       "       [1.59825772e-01, 2.62367189e-01, 1.90992743e-01],\n",
       "       [3.84449959e-06, 1.55265033e-02, 2.45181024e-02],\n",
       "       [3.07040095e-01, 2.75042593e-01, 3.15119326e-01],\n",
       "       [4.53963876e-03, 1.63851082e-02, 6.63921058e-01],\n",
       "       [5.32593012e-01, 6.86907172e-02, 6.38554096e-02],\n",
       "       [1.53311938e-01, 4.17721033e-01, 3.26649666e-01],\n",
       "       [9.91190374e-02, 1.18524671e-01, 2.25625962e-01],\n",
       "       [2.46340871e-01, 2.77919650e-01, 1.24335021e-01],\n",
       "       [6.43280149e-02, 1.46913677e-01, 1.55981541e-01],\n",
       "       [1.61018968e-01, 3.26450109e-01, 2.81742692e-01],\n",
       "       [3.33493948e-03, 5.07443249e-02, 7.86661208e-02],\n",
       "       [4.44616675e-01, 1.77754432e-01, 1.63266212e-01],\n",
       "       [2.05028504e-01, 2.81039834e-01, 2.44913489e-01],\n",
       "       [3.88026655e-01, 2.20481753e-01, 1.32419109e-01],\n",
       "       [2.65425116e-01, 2.39104539e-01, 1.70544863e-01],\n",
       "       [3.51843596e-01, 2.42539465e-01, 9.67302620e-02],\n",
       "       [1.94045573e-01, 2.05223411e-01, 1.33502573e-01],\n",
       "       [1.54904902e-01, 3.32626164e-01, 2.80331671e-01],\n",
       "       [7.69119561e-02, 4.18558300e-01, 4.53512758e-01],\n",
       "       [2.77283639e-01, 1.37013853e-01, 1.63214773e-01],\n",
       "       [3.35895061e-01, 3.11708719e-01, 2.15251505e-01],\n",
       "       [1.42923564e-01, 1.62064135e-01, 1.18339866e-01],\n",
       "       [5.68751693e-02, 5.19591868e-02, 4.83362079e-01],\n",
       "       [2.99112022e-01, 2.00778782e-01, 2.84650564e-01],\n",
       "       [3.25491369e-01, 2.67032206e-01, 1.74711436e-01],\n",
       "       [8.79027843e-02, 4.55941081e-01, 1.28469110e-01],\n",
       "       [3.15948337e-01, 2.77754277e-01, 1.70544505e-01],\n",
       "       [1.28665447e-01, 2.47753710e-01, 2.12031841e-01],\n",
       "       [1.65784717e-01, 3.64729166e-01, 3.11210275e-01],\n",
       "       [2.13019907e-01, 1.99404597e-01, 2.01248318e-01],\n",
       "       [3.90630066e-01, 2.83400416e-01, 1.29235715e-01],\n",
       "       [1.49895847e-02, 3.96385789e-02, 1.57316566e-01],\n",
       "       [3.44555974e-01, 2.81378329e-01, 2.36002445e-01],\n",
       "       [1.14006996e-02, 5.38734794e-02, 8.92998874e-02],\n",
       "       [1.42158538e-01, 3.06731731e-01, 3.49995077e-01],\n",
       "       [1.19861811e-01, 4.14053351e-01, 4.37524319e-01],\n",
       "       [2.18204468e-01, 2.51120329e-01, 3.18257213e-01],\n",
       "       [2.47731864e-01, 3.18531036e-01, 4.03101742e-01],\n",
       "       [5.57658076e-02, 3.07770789e-01, 7.39819705e-02],\n",
       "       [3.50419164e-01, 2.99471855e-01, 3.15569997e-01],\n",
       "       [2.19397902e-01, 5.06934345e-01, 7.79339969e-02],\n",
       "       [1.42720699e-01, 2.26332814e-01, 1.12953037e-01],\n",
       "       [1.70412749e-01, 3.73093665e-01, 3.65574419e-01],\n",
       "       [3.65749300e-02, 1.53432220e-01, 2.90107071e-01],\n",
       "       [4.73356247e-03, 1.13542795e-01, 2.46192575e-01],\n",
       "       [5.34290671e-02, 1.46872908e-01, 1.67853028e-01],\n",
       "       [2.00923085e-01, 3.36361706e-01, 1.82995945e-01],\n",
       "       [3.80167603e-01, 2.34448671e-01, 7.38565624e-02],\n",
       "       [3.15829277e-01, 1.82536066e-01, 1.22165084e-01],\n",
       "       [3.28483164e-01, 1.60765260e-01, 1.96293741e-01],\n",
       "       [2.74906635e-01, 2.41308570e-01, 1.71369940e-01],\n",
       "       [4.00720000e-01, 1.59258515e-01, 6.35621250e-02],\n",
       "       [2.39468217e-02, 6.76491261e-02, 3.19653928e-01],\n",
       "       [3.28500122e-01, 3.05740565e-01, 1.11135781e-01],\n",
       "       [3.53979170e-02, 3.53291035e-02, 1.75163031e-01],\n",
       "       [6.97275698e-02, 2.18323439e-01, 6.31753802e-02],\n",
       "       [1.07884675e-01, 6.01848006e-01, 9.77462232e-02],\n",
       "       [1.49170011e-01, 1.35205150e-01, 1.50121778e-01],\n",
       "       [1.93203688e-01, 2.97045559e-01, 3.00991058e-01],\n",
       "       [4.24723923e-02, 2.57691771e-01, 8.22297335e-02],\n",
       "       [2.47125506e-01, 3.01036239e-01, 2.07399189e-01],\n",
       "       [3.00013989e-01, 2.84132659e-01, 2.01192468e-01],\n",
       "       [6.69067204e-02, 2.39189535e-01, 2.37743497e-01],\n",
       "       [2.19735861e-01, 6.64548576e-02, 1.69866890e-01],\n",
       "       [1.35119140e-01, 3.69297922e-01, 4.34425920e-01],\n",
       "       [1.56556070e-02, 7.71661103e-02, 5.58735788e-01],\n",
       "       [1.80288374e-01, 2.89403856e-01, 2.21588522e-01],\n",
       "       [3.55876088e-01, 3.23866278e-01, 3.04258317e-01],\n",
       "       [1.48981810e-04, 8.05687904e-03, 1.06427997e-01],\n",
       "       [1.57487094e-01, 2.95358390e-01, 2.08784878e-01],\n",
       "       [6.11215234e-02, 1.75276726e-01, 1.60458952e-01],\n",
       "       [1.15176737e-01, 3.02793205e-01, 1.77067518e-01],\n",
       "       [2.48684376e-01, 1.17394179e-01, 1.98201180e-01],\n",
       "       [7.30093896e-01, 2.93902367e-01, 8.00883472e-02],\n",
       "       [1.66600585e-01, 3.23647559e-01, 2.04016328e-01],\n",
       "       [2.36643523e-01, 3.69233489e-01, 2.18533456e-01],\n",
       "       [1.28121018e-01, 3.62999111e-01, 4.16012228e-01],\n",
       "       [2.36444086e-01, 2.05534667e-01, 1.84955686e-01],\n",
       "       [1.61886424e-01, 3.74960721e-01, 3.46719921e-01],\n",
       "       [8.14774036e-02, 8.27095807e-02, 1.73079282e-01],\n",
       "       [6.37810826e-02, 1.97928429e-01, 1.18556112e-01],\n",
       "       [2.01143056e-01, 3.05894911e-01, 2.29536504e-01],\n",
       "       [7.86215663e-02, 1.53173506e-02, 4.93404567e-02],\n",
       "       [1.08813047e-01, 3.28255713e-01, 1.89547628e-01],\n",
       "       [2.02851444e-01, 1.28098965e-01, 6.95635378e-02],\n",
       "       [3.20605487e-01, 2.95819372e-01, 1.09513670e-01],\n",
       "       [2.09522247e-02, 1.38576895e-01, 1.32511646e-01],\n",
       "       [2.62684286e-01, 1.48130417e-01, 2.36780703e-01],\n",
       "       [1.95238680e-01, 1.46676362e-01, 1.52647197e-01],\n",
       "       [3.27825546e-05, 3.18663418e-02, 7.85160065e-03],\n",
       "       [2.80489653e-01, 4.26270992e-01, 7.81960785e-02],\n",
       "       [3.82156849e-01, 3.02023947e-01, 1.10998869e-01],\n",
       "       [2.63603330e-01, 3.12807024e-01, 1.56253874e-01],\n",
       "       [3.93727750e-01, 8.31711590e-02, 1.14093095e-01],\n",
       "       [3.46422195e-04, 2.62419581e-02, 1.13803864e-01],\n",
       "       [6.71342015e-03, 6.36830926e-02, 2.59221733e-01],\n",
       "       [2.47892737e-03, 4.80564535e-02, 1.76528007e-01],\n",
       "       [2.73211628e-01, 3.24273825e-01, 2.35734820e-01],\n",
       "       [3.72051507e-01, 3.56047481e-01, 1.08134359e-01],\n",
       "       [1.67968363e-01, 3.60647857e-01, 3.93980175e-01],\n",
       "       [1.82697982e-01, 2.79596329e-01, 1.11936271e-01],\n",
       "       [3.25202942e-02, 2.85266340e-02, 5.11408150e-01],\n",
       "       [5.93751669e-04, 2.88843513e-02, 2.67883390e-01],\n",
       "       [1.40793413e-01, 3.21934760e-01, 2.36615330e-01],\n",
       "       [3.32329273e-01, 2.11288303e-01, 1.05696619e-01],\n",
       "       [2.28145748e-01, 2.35156924e-01, 2.76200056e-01],\n",
       "       [2.05818027e-01, 2.90331721e-01, 1.59255862e-01],\n",
       "       [8.02087784e-03, 1.97567314e-01, 5.58951497e-03],\n",
       "       [6.38813078e-02, 4.94364470e-01, 1.82844579e-01],\n",
       "       [2.33262777e-03, 1.42438173e-01, 1.27065927e-01],\n",
       "       [3.75147462e-02, 2.93146789e-01, 6.63111508e-02],\n",
       "       [3.70317101e-02, 2.99790204e-01, 7.11022615e-02],\n",
       "       [2.02327609e-01, 2.31880844e-02, 7.20787644e-02],\n",
       "       [2.63069868e-01, 1.79173470e-01, 1.36262506e-01],\n",
       "       [3.81494701e-01, 1.85448974e-01, 1.23979241e-01],\n",
       "       [1.07736766e-01, 1.49996519e-01, 1.68794364e-01],\n",
       "       [1.81244999e-01, 3.52091610e-01, 2.76952147e-01],\n",
       "       [2.32606262e-01, 1.22314990e-01, 1.71033353e-01],\n",
       "       [1.28746033e-05, 1.39069498e-01, 2.70164013e-03],\n",
       "       [1.80300772e-02, 1.62774861e-01, 3.83010834e-01],\n",
       "       [3.38239908e-01, 1.86171383e-01, 1.50157899e-01],\n",
       "       [1.09471470e-01, 3.31903428e-01, 3.34421217e-01],\n",
       "       [4.44892347e-01, 4.02008176e-01, 2.51675189e-01],\n",
       "       [4.10375088e-01, 1.81468725e-01, 1.65509731e-01],\n",
       "       [6.70115948e-02, 3.43917161e-01, 1.70962602e-01],\n",
       "       [1.92805886e-01, 4.55511481e-01, 1.81069672e-01],\n",
       "       [1.48897618e-01, 4.16291088e-01, 4.44760829e-01],\n",
       "       [9.23348665e-02, 3.57672602e-01, 3.88673693e-01],\n",
       "       [2.54942298e-01, 2.12344646e-01, 1.13094687e-01],\n",
       "       [1.06596589e-01, 2.97359705e-01, 2.49875367e-01],\n",
       "       [1.98612124e-01, 4.57597077e-02, 2.78905928e-02],\n",
       "       [1.84335053e-01, 2.43356735e-01, 2.28735089e-01],\n",
       "       [1.74257338e-01, 2.67493784e-01, 2.39967644e-01],\n",
       "       [1.93371505e-01, 3.29662561e-01, 4.15139318e-01],\n",
       "       [7.47907162e-03, 6.53182864e-02, 4.21222746e-02],\n",
       "       [4.41505700e-01, 2.28093147e-01, 1.37060672e-01],\n",
       "       [2.89689898e-02, 1.11440152e-01, 8.08207393e-02],\n",
       "       [2.67278373e-01, 2.18219548e-01, 2.65106052e-01],\n",
       "       [1.66221529e-01, 3.14124554e-01, 1.57087833e-01],\n",
       "       [3.19593489e-01, 3.27143729e-01, 1.20715976e-01],\n",
       "       [5.31577468e-02, 3.80190015e-01, 7.86635280e-02],\n",
       "       [4.81653810e-02, 3.13349038e-01, 5.03507555e-02],\n",
       "       [9.55881476e-02, 2.23803163e-01, 3.84413898e-02],\n",
       "       [2.78744578e-01, 1.92213893e-01, 2.47079492e-01],\n",
       "       [5.45947254e-02, 3.19104791e-01, 1.40266657e-01],\n",
       "       [1.11502409e-03, 1.72817409e-02, 1.58863068e-02],\n",
       "       [3.35290372e-01, 1.65601939e-01, 1.57165468e-01],\n",
       "       [1.63378417e-01, 4.01170850e-01, 4.54354882e-01],\n",
       "       [1.71086192e-01, 3.82846177e-01, 2.00955003e-01],\n",
       "       [2.69768625e-01, 2.76904523e-01, 1.84531927e-01],\n",
       "       [2.83669889e-01, 2.37021446e-01, 1.96222246e-01],\n",
       "       [1.26947075e-01, 4.01952744e-01, 4.66094255e-01],\n",
       "       [8.92122090e-02, 3.86822224e-02, 5.47696650e-02],\n",
       "       [2.54844576e-01, 1.40256375e-01, 8.52698982e-02],\n",
       "       [3.27962905e-01, 2.16469496e-01, 1.31616801e-01],\n",
       "       [1.80623621e-01, 2.89380312e-01, 2.21517384e-01],\n",
       "       [3.01965296e-01, 2.29089946e-01, 1.98350847e-01],\n",
       "       [3.20790082e-01, 3.49020660e-01, 2.00902253e-01],\n",
       "       [9.93619561e-02, 2.72084117e-01, 9.22976136e-02],\n",
       "       [7.15812147e-02, 7.24729598e-02, 1.53786033e-01],\n",
       "       [9.12618339e-02, 3.76801252e-01, 4.01979208e-01],\n",
       "       [3.76557171e-01, 1.35949373e-01, 7.84724057e-02],\n",
       "       [3.29690129e-01, 2.67212391e-01, 1.54257655e-01],\n",
       "       [1.85732514e-01, 3.72633308e-01, 3.12450767e-01],\n",
       "       [1.91028774e-01, 4.20762599e-01, 1.33787334e-01],\n",
       "       [6.95379078e-02, 1.00432187e-01, 7.22237229e-02],\n",
       "       [1.03555739e-01, 3.05354714e-01, 4.07687426e-02],\n",
       "       [1.91718549e-01, 4.54157770e-01, 2.39016086e-01],\n",
       "       [1.79987550e-01, 2.53742814e-01, 2.50378817e-01],\n",
       "       [1.07533365e-01, 3.72064412e-01, 4.40530747e-01],\n",
       "       [2.23802924e-01, 2.76423633e-01, 2.86999643e-01],\n",
       "       [2.66005605e-01, 2.90408254e-01, 2.27424175e-01],\n",
       "       [1.85596704e-01, 3.69411767e-01, 2.48258024e-01],\n",
       "       [3.04043293e-03, 9.74725187e-02, 6.62897706e-01],\n",
       "       [4.05050129e-01, 1.97844148e-01, 1.24045461e-01],\n",
       "       [3.02832961e-01, 2.05049247e-01, 2.55799174e-01],\n",
       "       [9.89663601e-03, 4.11133170e-02, 3.00610840e-01],\n",
       "       [2.10615456e-01, 2.89708078e-01, 3.88495952e-01],\n",
       "       [1.88339263e-01, 2.05237836e-01, 1.07574075e-01],\n",
       "       [3.54558289e-01, 1.59411609e-01, 1.10167474e-01],\n",
       "       [2.73410708e-01, 2.26462722e-01, 2.51355231e-01],\n",
       "       [3.53092074e-01, 2.54307210e-01, 1.50396585e-01],\n",
       "       [4.27668571e-01, 4.12973732e-01, 1.58296317e-01],\n",
       "       [1.67281240e-01, 3.30729455e-01, 3.90006721e-01],\n",
       "       [2.34759748e-01, 2.85145164e-01, 2.94990867e-01],\n",
       "       [3.61395955e-01, 2.71269619e-01, 1.40237510e-01],\n",
       "       [4.14414227e-01, 2.02444702e-01, 1.55953765e-01],\n",
       "       [1.90604955e-01, 3.51643682e-01, 2.90336728e-01],\n",
       "       [3.75568867e-04, 6.95693791e-02, 1.20458007e-03],\n",
       "       [2.73677766e-01, 2.57112980e-01, 2.14289963e-01],\n",
       "       [3.67280543e-01, 3.01685780e-01, 1.99890107e-01],\n",
       "       [4.30130064e-02, 2.37856477e-01, 3.66428494e-02],\n",
       "       [2.18072444e-01, 2.13378519e-01, 1.96134329e-01],\n",
       "       [2.11290479e-01, 3.80958825e-01, 2.85257518e-01],\n",
       "       [2.87980378e-01, 1.47904783e-01, 1.62087619e-01],\n",
       "       [2.48538136e-01, 4.70329702e-01, 2.72313058e-01],\n",
       "       [4.50432330e-01, 3.35607201e-01, 6.05324507e-02],\n",
       "       [3.68458539e-01, 1.62122637e-01, 9.53566432e-02],\n",
       "       [6.32651448e-02, 2.35754699e-01, 9.22132432e-02],\n",
       "       [4.76837158e-06, 1.31986648e-01, 5.18530607e-04],\n",
       "       [4.35729563e-01, 4.34450120e-01, 2.74281204e-01],\n",
       "       [3.92392218e-01, 3.52088809e-01, 2.14681745e-01],\n",
       "       [1.16000563e-01, 2.73460209e-01, 2.25086391e-01],\n",
       "       [7.11023510e-02, 3.41003954e-01, 2.11612552e-01],\n",
       "       [6.95050955e-02, 3.38927299e-01, 1.09988719e-01],\n",
       "       [4.04188633e-01, 7.57107139e-02, 3.85756493e-02],\n",
       "       [2.32436150e-01, 2.76511699e-01, 1.63250864e-01],\n",
       "       [1.93815529e-02, 3.22752953e-01, 1.23748958e-01],\n",
       "       [4.09138769e-01, 2.61308581e-01, 8.58514309e-02],\n",
       "       [6.26342893e-02, 2.57920623e-01, 9.50012207e-02],\n",
       "       [2.21659750e-01, 3.13449383e-01, 1.10795796e-01],\n",
       "       [2.32450366e-02, 1.43807858e-01, 7.68544674e-02],\n",
       "       [1.19209290e-07, 5.48137426e-02, 5.55932522e-04],\n",
       "       [1.48287147e-01, 2.97177285e-01, 2.08021909e-01],\n",
       "       [4.30411100e-02, 3.62648904e-01, 3.42351139e-01],\n",
       "       [1.48518503e-01, 4.19683695e-01, 3.57165575e-01],\n",
       "       [6.20782375e-05, 3.75559926e-03, 1.09989464e-01],\n",
       "       [1.81995213e-01, 2.71512330e-01, 2.83541322e-01],\n",
       "       [2.43213296e-01, 2.94599235e-01, 3.28521580e-01],\n",
       "       [8.44456255e-02, 5.37056327e-02, 1.84714198e-01],\n",
       "       [2.57862389e-01, 2.72258162e-01, 2.74661422e-01],\n",
       "       [2.35237241e-01, 2.62113214e-01, 2.99421549e-01],\n",
       "       [3.49346101e-01, 1.93212628e-01, 1.36858404e-01],\n",
       "       [1.36601448e-01, 2.98605323e-01, 3.27861488e-01],\n",
       "       [3.42779398e-01, 2.68411040e-01, 2.14521050e-01],\n",
       "       [1.61103338e-01, 2.77330518e-01, 2.25846142e-01],\n",
       "       [1.51849061e-01, 1.66153044e-01, 1.84878290e-01],\n",
       "       [1.36022210e-01, 3.54693234e-01, 2.85832286e-01],\n",
       "       [1.89681917e-01, 2.47406304e-01, 1.65363818e-01],\n",
       "       [2.99345911e-01, 3.13807726e-01, 2.55457461e-01],\n",
       "       [5.92903495e-01, 2.00537473e-01, 7.81013072e-02],\n",
       "       [9.41597521e-02, 3.58029783e-01, 4.06102151e-01],\n",
       "       [1.38729304e-01, 2.35598505e-01, 1.55520618e-01],\n",
       "       [2.70235300e-01, 1.91696495e-01, 3.24358642e-01],\n",
       "       [2.40328223e-01, 1.95756078e-01, 2.44223714e-01],\n",
       "       [3.14208269e-01, 3.15987527e-01, 1.41451240e-01],\n",
       "       [2.47920156e-01, 3.67465258e-01, 2.37168223e-01],\n",
       "       [1.04976892e-02, 7.86839128e-02, 2.72186428e-01],\n",
       "       [5.28829277e-01, 1.03096247e-01, 4.54948545e-02],\n",
       "       [1.12533152e-01, 3.85254204e-01, 4.50387478e-01],\n",
       "       [2.54909426e-01, 1.78949922e-01, 9.87994075e-02],\n",
       "       [3.52691919e-01, 2.57892370e-01, 1.90116167e-01],\n",
       "       [6.25508428e-02, 1.21608168e-01, 2.28589267e-01],\n",
       "       [2.95096874e-01, 1.81936473e-01, 1.24336720e-01],\n",
       "       [2.17524588e-01, 1.36565894e-01, 1.33291036e-01],\n",
       "       [2.50837356e-01, 4.64228988e-01, 7.86945224e-02],\n",
       "       [9.39925015e-02, 3.79038930e-01, 4.01450157e-01],\n",
       "       [2.17974186e-03, 4.89535332e-02, 1.69607997e-03],\n",
       "       [2.21080422e-01, 2.61682004e-01, 1.51124746e-01],\n",
       "       [1.22173727e-02, 5.97227812e-02, 2.90613234e-01],\n",
       "       [3.48790884e-02, 4.14917499e-01, 1.69259816e-01],\n",
       "       [6.07949495e-03, 2.96802223e-02, 1.87957585e-01],\n",
       "       [3.32770526e-01, 2.68940359e-01, 1.59591675e-01],\n",
       "       [2.12663263e-01, 3.95687699e-01, 2.42972910e-01],\n",
       "       [4.06712294e-02, 7.52932429e-02, 3.57294798e-01],\n",
       "       [2.06659883e-01, 3.60242248e-01, 1.39351666e-01],\n",
       "       [2.47359276e-06, 1.40586495e-03, 4.57296968e-01],\n",
       "       [3.47392440e-01, 2.41734087e-01, 2.30731606e-01],\n",
       "       [3.54906082e-01, 2.90398985e-01, 1.13419503e-01],\n",
       "       [2.88762182e-01, 3.21366251e-01, 3.07455182e-01],\n",
       "       [1.58418804e-01, 3.59467447e-01, 3.72298956e-01],\n",
       "       [1.08788013e-01, 1.23258859e-01, 1.32110089e-01],\n",
       "       [3.36868256e-01, 3.08999300e-01, 2.80854791e-01],\n",
       "       [1.93702877e-01, 3.38275909e-01, 2.06911564e-01],\n",
       "       [1.73228353e-01, 3.22460681e-01, 3.54677528e-01],\n",
       "       [2.75344610e-01, 3.59080404e-01, 4.42980826e-02],\n",
       "       [1.72214359e-01, 3.01561922e-01, 3.78389686e-01],\n",
       "       [2.89346099e-01, 2.60496616e-01, 2.04788446e-01],\n",
       "       [6.11669421e-02, 2.13978082e-01, 9.61654186e-02],\n",
       "       [7.81509280e-03, 2.44986713e-02, 4.48679149e-01],\n",
       "       [3.03772748e-01, 4.74270225e-01, 2.11382836e-01],\n",
       "       [4.18469310e-02, 2.78589070e-01, 2.80827880e-01],\n",
       "       [1.91220492e-01, 1.07291013e-01, 1.03948772e-01],\n",
       "       [4.05218840e-01, 2.12567091e-01, 8.31765831e-02],\n",
       "       [1.49354190e-01, 3.35937083e-01, 4.21379924e-01],\n",
       "       [1.65745437e-01, 3.06443274e-01, 3.79980773e-01],\n",
       "       [4.27256107e-01, 9.55602527e-02, 1.33080870e-01],\n",
       "       [3.48985493e-02, 2.56950200e-01, 1.37731254e-01],\n",
       "       [1.26421452e-03, 7.61255622e-03, 2.97270417e-01],\n",
       "       [2.47032374e-01, 2.51649678e-01, 2.34365255e-01],\n",
       "       [4.00369167e-02, 3.73299599e-01, 1.49654448e-01],\n",
       "       [4.35920298e-01, 1.40097111e-01, 7.75139034e-02],\n",
       "       [2.09282041e-01, 2.78540879e-01, 3.96261513e-01],\n",
       "       [1.27687931e-01, 3.21752459e-01, 1.26328140e-01],\n",
       "       [1.23119354e-03, 4.14439738e-02, 4.08626199e-02],\n",
       "       [2.36902058e-01, 1.90307438e-01, 1.18628711e-01],\n",
       "       [3.49334300e-01, 2.02815413e-01, 1.32022232e-01],\n",
       "       [3.19580257e-01, 2.03048766e-01, 1.20664150e-01],\n",
       "       [1.09805197e-01, 3.28960747e-01, 3.45232725e-01],\n",
       "       [1.63072944e-02, 1.89130634e-01, 3.49210173e-01],\n",
       "       [2.35551924e-01, 2.85131961e-01, 2.81356603e-01],\n",
       "       [8.92767310e-02, 6.30332008e-02, 1.04902655e-01],\n",
       "       [1.81330994e-01, 1.18882284e-02, 2.85655726e-02]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   varible_name  perturbation_effect\n",
      "17      컴퓨터/인터넷             0.105500\n",
      "6            기타             0.067070\n",
      "9       비즈니스/쇼핑             0.064833\n",
      "0    세션당 방문사이트수             0.037320\n",
      "7        뉴스/미디어             0.025902\n",
      "1        세션지속시간             0.016763\n",
      "8         문화/예술             0.014035\n",
      "3            게임             0.005174\n",
      "11       스포츠/레저             0.005075\n",
      "15        정치/사회             0.002527\n",
      "10        생활/건강             0.002033\n",
      "2     쇼핑사이트방문여부             0.001977\n",
      "5         교육/학교             0.001861\n",
      "4        경제/재테크             0.001658\n",
      "13        연예/오락             0.000948\n",
      "18        학문/사전             0.000616\n",
      "14           인물             0.000532\n",
      "12      여행/세계정보             0.000301\n",
      "16           종교             0.000023\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(X_train.reshape(1606*5000, 19))\n",
    "train_df.columns = ['세션당 방문사이트수','세션지속시간','쇼핑사이트방문여부' ,\n",
    "                    '게임', '경제/재테크', '교육/학교', '기타', '뉴스/미디어', '문화/예술', '비즈니스/쇼핑', '생활/건강', '스포츠/레저',\n",
    " '여행/세계정보', '연예/오락', '인물', '정치/사회', '종교', '컴퓨터/인터넷', '학문/사전']\n",
    "## Sensitivity Analysis\n",
    "importance_df = pd.DataFrame(np.zeros((19, 2)), columns=['varible_name', 'perturbation_effect'])\n",
    "def var_importance(model):\n",
    "    x = X_test # Get a sample of data\n",
    "    orig_out = model.predict(x)\n",
    "    for i in range(19):  # iterate over the three features\n",
    "        new_x = x.copy()\n",
    "        perturbation = np.random.normal(0.0, 0.2, size=new_x.shape[:2])\n",
    "        new_x[:, :, i] = new_x[:, :, i] + perturbation\n",
    "        perturbed_out = model.predict(new_x)\n",
    "        effect = ((orig_out - perturbed_out) ** 2).mean() ** 0.5 #RMSE\n",
    "        importance_df.iloc[i,0] = train_df.columns[i]\n",
    "        importance_df.iloc[i,1] = effect\n",
    "        #print(f'Variable {i+1}, perturbation effect: {effect:.4f}')\n",
    "var_importance(model)\n",
    "importance_df = importance_df.sort_values(by='perturbation_effect', ascending=False)\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perm = PermutationImportance(model, random_state=1).fit(X_test,y_test)\n",
    "#eli5.show_weights(perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6023222208023071\n"
     ]
    }
   ],
   "source": [
    "#가전제품 f1-score: 0.7819220343082984\n",
    "#패션 accuracy: 0.7188029361269287\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 중순"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X_중순, Y_중순, test_size=0.3, random_state=42)\n",
    "#y_train = to_categorical(y_train)\n",
    "#y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#쇼핑사이트 방문횟수, 문화/예술, 비즈니스/쇼핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_2 (Masking)          (None, 5000, 19)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                21504     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 24,163\n",
      "Trainable params: 24,163\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#masking layer 추가(https://stackoverflow.com/questions/49670832/keras-lstm-with-masking-layer-for-variable-length-inputs)\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0., input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(64,input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "#model.add(Glob)\n",
    "model.add(Dense(32, activation= 'relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation= 'relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr= 0.001, rho = 0.9))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 744 samples, validate on 319 samples\n",
      "Epoch 1/30\n",
      " - 21s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 2/30\n",
      " - 21s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 3/30\n",
      " - 21s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 4/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 5/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 6/30\n",
      " - 21s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 7/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 8/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 9/30\n",
      " - 21s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 10/30\n",
      " - 21s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 11/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 12/30\n",
      " - 21s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 13/30\n",
      " - 21s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 14/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 15/30\n",
      " - 31s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 16/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 17/30\n",
      " - 21s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 18/30\n",
      " - 21s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 19/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 20/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 21/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 22/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 23/30\n",
      " - 21s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 24/30\n",
      " - 21s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 25/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 26/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 27/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 28/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 29/30\n",
      " - 21s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 30/30\n",
      " - 20s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, batch_size=200, validation_data=(X_test, y_test), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "preds[preds>=0.5] = 1\n",
    "preds[preds<0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 0., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   varible_name  perturbation_effect\n",
      "6            기타             0.019017\n",
      "0    세션당 방문사이트수             0.018900\n",
      "9       비즈니스/쇼핑             0.017860\n",
      "17      컴퓨터/인터넷             0.011322\n",
      "1        세션지속시간             0.005882\n",
      "3            게임             0.004853\n",
      "2     쇼핑사이트방문여부             0.002543\n",
      "7        뉴스/미디어             0.002450\n",
      "15        정치/사회             0.002309\n",
      "5         교육/학교             0.001170\n",
      "4        경제/재테크             0.000860\n",
      "16           종교             0.000262\n",
      "11       스포츠/레저             0.000238\n",
      "10        생활/건강             0.000212\n",
      "12      여행/세계정보             0.000210\n",
      "18        학문/사전             0.000137\n",
      "13        연예/오락             0.000118\n",
      "8         문화/예술             0.000115\n",
      "14           인물             0.000049\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(X_train.reshape(X_train.shape[0] * X_train.shape[1], 19))\n",
    "train_df.columns = ['세션당 방문사이트수','세션지속시간','쇼핑사이트방문여부' ,\n",
    "                    '게임', '경제/재테크', '교육/학교', '기타', '뉴스/미디어', '문화/예술', '비즈니스/쇼핑', '생활/건강', '스포츠/레저',\n",
    " '여행/세계정보', '연예/오락', '인물', '정치/사회', '종교', '컴퓨터/인터넷', '학문/사전']\n",
    "\n",
    "## Sensitivity Analysis\n",
    "importance_df = pd.DataFrame(np.zeros((19, 2)), columns=['varible_name', 'perturbation_effect'])\n",
    "def var_importance(model):\n",
    "    x = X_test # Get a sample of data\n",
    "    orig_out = model.predict(x)\n",
    "    for i in range(19):  # iterate over the three features\n",
    "        new_x = x.copy()\n",
    "        perturbation = np.random.normal(0.0, 0.2, size=new_x.shape[:2])\n",
    "        new_x[:, :, i] = new_x[:, :, i] + perturbation\n",
    "        perturbed_out = model.predict(new_x)\n",
    "        effect = ((orig_out - perturbed_out) ** 2).mean() ** 0.5 #RMSE\n",
    "        importance_df.iloc[i,0] = train_df.columns[i]\n",
    "        importance_df.iloc[i,1] = effect\n",
    "        #print(f'Variable {i+1}, perturbation effect: {effect:.4f}')\n",
    "var_importance(model)\n",
    "importance_df = importance_df.sort_values(by='perturbation_effect', ascending=False)\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()                                       \n",
    "plt.show()                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.632478654384613\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 하순"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X_하순, Y_하순, test_size=0.3, random_state=42)\n",
    "#y_train = to_categorical(y_train)\n",
    "#y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-f2d3bf183aa0>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-f2d3bf183aa0>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    model.add(Dense(, activation='sigmoid'))\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#masking layer 추가(https://stackoverflow.com/questions/49670832/keras-lstm-with-masking-layer-for-variable-length-inputs)\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0., input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(64,input_shape = (X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(32, activation= 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation= 'relu'))\n",
    "model.add(Dense(, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr= 0.001, rho = 0.9))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 744 samples, validate on 319 samples\n",
      "Epoch 1/60\n",
      " - 28s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 2/60\n",
      " - 27s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 3/60\n",
      " - 27s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 4/60\n",
      " - 27s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 5/60\n",
      " - 26s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 6/60\n",
      " - 26s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 7/60\n",
      " - 27s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 8/60\n",
      " - 27s - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 9/60\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[5000,500,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node training_1/RMSprop/gradients/lstm_2/strided_slice_18_grad/StridedSliceGrad}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-43a24a7a064d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[5000,500,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node training_1/RMSprop/gradients/lstm_2/strided_slice_18_grad/StridedSliceGrad}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=60, batch_size=500, validation_data=(X_test, y_test), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(X_train.reshape(X_train.shape[0] * X_train.shape[1], 19))\n",
    "train_df.columns = ['세션당 방문사이트수','세션지속시간','쇼핑사이트방문여부' ,\n",
    "                    '게임', '경제/재테크', '교육/학교', '기타', '뉴스/미디어', '문화/예술', '비즈니스/쇼핑', '생활/건강', '스포츠/레저',\n",
    " '여행/세계정보', '연예/오락', '인물', '정치/사회', '종교', '컴퓨터/인터넷', '학문/사전']\n",
    "## Sensitivity Analysis\n",
    "importance_df = pd.DataFrame(np.zeros((19, 2)), columns=['varible_name', 'perturbation_effect'])\n",
    "def var_importance(model):\n",
    "    x = X_test # Get a sample of data\n",
    "    orig_out = model.predict(x)\n",
    "    for i in range(19):  # iterate over the three features\n",
    "        new_x = x.copy()\n",
    "        perturbation = np.random.normal(0.0, 0.2, size=new_x.shape[:2])\n",
    "        new_x[:, :, i] = new_x[:, :, i] + perturbation\n",
    "        perturbed_out = model.predict(new_x)\n",
    "        effect = ((orig_out - perturbed_out) ** 2).mean() ** 0.5 #RMSE\n",
    "        importance_df.iloc[i,0] = train_df.columns[i]\n",
    "        importance_df.iloc[i,1] = effect\n",
    "        #print(f'Variable {i+1}, perturbation effect: {effect:.4f}')\n",
    "var_importance(model)\n",
    "importance_df = importance_df.sort_values(by='perturbation_effect', ascending=False)\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()                                       \n",
    "plt.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
